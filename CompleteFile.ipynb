{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Imports \"\"\"\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"Global definitons\"\"\"\n",
    "_start = 'S_START'\n",
    "_end = 'S_END'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" util definitions\"\"\"\n",
    "\n",
    "def hyperbolic(net):\n",
    "    return np.tanh(net)\n",
    "\n",
    "def relu(net):\n",
    "    return np.maximum(0,net)\n",
    "\n",
    "def softmax(net):\n",
    "    _exp = np.exp(net)\n",
    "    return _exp/np.sum(_exp)\n",
    "\n",
    "def predict(scores):\n",
    "    return np.argmax(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WordItem:\n",
    "    def __init__(self,word,count=0):\n",
    "        self.word = word\n",
    "        self.count = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RNNlayer:\n",
    "    \n",
    "    \"\"\" \n",
    "    RNN nodes for decoder\n",
    "    \n",
    "    hidden state at time step t of decoder is conditioned on hidden state at time step t-1,\n",
    "    output at time step t-1 and input at time step t\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, inputSize, outputSize, bptt_truncate = 5, hiddenDim = 10):\n",
    "        \"\"\"\n",
    "        inputSize = dimensions of the input embedding \n",
    "        outputSize = vocabulary size\n",
    "        hiddenDim = size of the hidden unit in RNN\n",
    "        bptt_truncate = truncate the number of time steps we calculate the gradient during backpropagation\n",
    "        \"\"\"\n",
    "        self.inputSize = inputSize\n",
    "        self.outputSize = outputSize\n",
    "        self.hiddenDim = hiddenDim\n",
    "        self. bptt_truncate = bptt_truncate\n",
    "        \n",
    "        self.w_in = np.random.uniform(-np.sqrt(1./inputSize), np.sqrt(1./inputSize),(hiddenDim, inputSize))\n",
    "        self.w_hh = np.random.uniform(-np.sqrt(1./hiddenDim), np.sqrt(1./hiddenDim),(hiddenDim, hiddenDim))\n",
    "        #self.w_outH = np.random.uniform(-np.sqrt(1./hiddenDim), np.sqrt(1./hiddenDim),(outputSize, hiddenDim))\n",
    "        self.w_out = np.random.uniform(-np.sqrt(1./hiddenDim), np.sqrt(1./hiddenDim),(outputSize, hiddenDim))\n",
    "        \n",
    "    def forwardProp(self, inSentence, expSent):\n",
    "        \"\"\"\n",
    "        inSentence: word indices in input language vocabulary\n",
    "        expSent: word indices in target language vocabulary\n",
    "        \"\"\"\n",
    "        \n",
    "        #Total number of time steps equal to number of words in the sentence\n",
    "        T = len(expSent)\n",
    "        \n",
    "        #Saving all hidden states and outputs during forward propagation\n",
    "        _h = np.zeros((T,self.hiddenDim))\n",
    "        _o = np.zeros((T,self.outputSize))\n",
    "        \n",
    "        #Initializing initial output as the start token\n",
    "        #_o[-1] = \n",
    "        \n",
    "        #For each time step calculating hidden state and output\n",
    "        for t in np.arange(T):\n",
    "            #outIdx = predict(_o[t-1])\n",
    "            _h[t] = hyperbolic(self.w_in.dot(inSentence[t]) + self.w_hh.dot(_h[t-1])) #+ self.w_outH[:,outIdx:outIdx+1])\n",
    "            _o[t] = softmax(self.w_out.dot(_h[t]))\n",
    "            \n",
    "        return _o, _h\n",
    "    \n",
    "    def calculateLoss(self, inSentence, expSentence):\n",
    "        \n",
    "        #For each sentence\n",
    "        o, h = self.forwardProp(inSentencecontext, expSentence)\n",
    "        #TODO recheck this part\n",
    "        correctPred = o[np.arange(len(expSentence)), expSentence]\n",
    "        #Loss for each sentence\n",
    "        l = -1 * np.sum(np.log(correctPred))\n",
    "        return l\n",
    "    \n",
    "    def calculateTotalLoss(self, inSentence, expSentences):\n",
    "        \n",
    "        L = 0.0\n",
    "        for i in len(inSentence):\n",
    "            L += self.calculateLoss(inSentencecontext[i], expSentences[i])\n",
    "            \n",
    "        return L\n",
    "    \n",
    "    def backPropTT(self, inSentence, expSentence):\n",
    "        \n",
    "        # Total number of time steps equal to number of words in the sentence\n",
    "        T = len(expSentence)\n",
    "        \n",
    "        # Performing forward propagation\n",
    "        o, h = self.forwardProp(inSentence, expSentence)\n",
    "        \n",
    "        # Defining gradient variables\n",
    "        dLdin = np.zeros(self.w_in.shape)\n",
    "        dLdhh = np.zeros(self.w_hh.shape)\n",
    "        #dLdoutH = np.zeros(self.w_outH.shape)\n",
    "        dLdout = np.zeros(self.w_out.shape)\n",
    "        \n",
    "        # Calculating the difference between output and actual output\n",
    "        delta_o = o\n",
    "        delta_o[np.arange(T), expSentence] -= 1\n",
    "        #print 'delta_o', delta_o\n",
    "        \n",
    "        # Calculating gradients backwards through time\n",
    "        for t in np.arange(T)[::-1]:\n",
    "            #Output gradient is only dependent on time step t\n",
    "            dLdout += np.outer(delta_o[t], h[t])\n",
    "            \n",
    "            # Initial delta calculation propagating gradients from output\n",
    "            delta_t = self.w_out.T.dot(delta_o[t]) * (1 - (h[t] ** 2))\n",
    "            \n",
    "            # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "            for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "                # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "                # Add to gradients at each previous step\n",
    "                dLdhh += np.outer(delta_t, h[bptt_step-1])              \n",
    "                dLdin += np.outer(delta_t, inSentence[bptt_step-1])\n",
    "                #dLdoutH += np.outer(delta_t, o[bptt_step-1])\n",
    "                # Update delta for next step dL/dz at t-1\n",
    "                delta_t = self.w_hh.T.dot(delta_t) * (1 - h[bptt_step-1] ** 2)\n",
    "            \"\"\"TODO review backprop implementation\"\"\"\n",
    "            \n",
    "        #return dLdin, dLdhh, dLdoutH, dLdout\n",
    "        return dLdin, dLdhh, dLdout\n",
    "    \n",
    "    def sgd_step(self, inSentence, expSentence, learningRate):\n",
    "        \n",
    "        \"\"\" Performs a single stochastic gradient step\"\"\"\n",
    "        \n",
    "        # Calculating gradients\n",
    "        #dLdin, dLdhh, dLdoutH, dLdout = self.backPropTT(inSentence, expSentence)\n",
    "        dLdin, dLdhh, dLdout = self.backPropTT(inSentence, expSentence)\n",
    "        \n",
    "        # Updating parameters\n",
    "        self.w_in -= learningRate * dLdin\n",
    "        self.w_hh -= learningRate * dLdhh\n",
    "        #self.w_outH -= learningRate * dLdoutH\n",
    "        self.w_out -= learningRate * dLdout\n",
    "        \n",
    "    def train_Decoder_With_SGD(self, X_train, Y_train, learningRate = 0.05, nepochs = 20):\n",
    "        \"\"\"TODO evaluate losses and update learning rate if required\"\"\"\n",
    "        for epoch in range(nepochs):\n",
    "            for i in range(len(Y_train)):\n",
    "                print i\n",
    "                self.sgd_step(X_train[i], Y_train[i], learningRate)\n",
    "                print 'W_in ', self.w_in\n",
    "                print 'W_hh ', self.w_hh\n",
    "                print 'W_out ', self.w_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Word preprocessing \"\"\"\n",
    "def dataset(_fi='/home/jazzycrazzy/PythonScripts/dataset.csv', _fo = 'testfile.txt'):\n",
    "    file_in = open(_fi)\n",
    "    #file_out = open(_fo,'wb')\n",
    "\n",
    "    words = [] #stores unique words encountered in the document as WordItem objects\n",
    "    _dict = {} #temporary dictionary to maintain count of each word\n",
    "    \n",
    "    _dict['UNK'] = 0\n",
    "\n",
    "    for l in file_in:\n",
    "        #file_out.write(l+'\\n')\n",
    "        l = _start+' '+l+' '+_end\n",
    "        split = word_tokenize(l.decode('utf-8'))\n",
    "        for w in split:\n",
    "            if len(w)==0:\n",
    "                continue\n",
    "            elif len(w) > 15: #if word's length is greater than 15 counting it as unknown\n",
    "                _dict['UNK'] += 1\n",
    "                continue\n",
    "            if w not in _dict:\n",
    "                _dict[w] = 1\n",
    "            _dict[w] += 1\n",
    "            \n",
    "    _vocab = {} #dictionary with words as keys and values as indices of them in 'word' list\n",
    "    _vocab['UNK'] = len(words)\n",
    "    words.append(WordItem('UNK',_dict['UNK']))\n",
    "    for k,v in _dict.iteritems():\n",
    "        if k != 'UNK':\n",
    "            _vocab[k] = len(words)\n",
    "            words.append(WordItem(k,v))\n",
    "        else:\n",
    "            words[0].count += 1\n",
    "    \n",
    "    #cleaning up unnecessary memory\n",
    "    del _dict\n",
    "    file_in.close()\n",
    "    #file_out.close()\n",
    "    \n",
    "    return _vocab, words\n",
    "\n",
    "def UnigramTable(_vocab, words):\n",
    "    \"\"\" Calculates probabilities based on count of each word present\"\"\"\n",
    "    pow = 0.75\n",
    "    totalFreqPow = 0.0\n",
    "    unigramTable = {}\n",
    "    \n",
    "    l = [words[i].count**pow for i in range(len(_vocab))]\n",
    "    totalFreqPow = np.sum(l)\n",
    "    \n",
    "    for i in range(len(_vocab)):\n",
    "        unigramTable[i] = (words[i].count**pow)/totalFreqPow\n",
    "    \n",
    "    del l\n",
    "    return unigramTable\n",
    "\n",
    "def hotVector(wordIndex,vocabSize):\n",
    "    \"\"\" Returns hot vector representation of a word \"\"\"\n",
    "    hVector = np.zeros(vocabSize)\n",
    "    hVector[wordIndex-1] = 1\n",
    "    return hVector\n",
    "\n",
    "def sigmoid(net):\n",
    "    \"\"\" Applies sigmoid logistic function on net \"\"\"\n",
    "    return 1.0/(1+np.exp(-net))\n",
    "\n",
    "def randomIdx(k, vocabSize, current):\n",
    "    \"\"\" Returns k indices from with unigram table randomly with respect to each word's probablity \"\"\"\n",
    "    global _unigramTable\n",
    "    idxs = list(np.random.choice(vocabSize, k+1, False, p = _unigramTable.values()))\n",
    "    if current in idxs:\n",
    "        idxs.remove(current)\n",
    "    else:\n",
    "        del idxs[-1]\n",
    "    return idxs\n",
    "    \n",
    "def softmaxCostGradient(net, target):\n",
    "    prob = softmax(net)\n",
    "    \n",
    "    \n",
    "def negSamplingCostGradient(out, context, emb, vocabSize, learningRate, W_Output, k = 10):\n",
    "    \n",
    "    #cost = []\n",
    "    errorHidden = np.zeros(shape=(emb.size,1))\n",
    "    \n",
    "    actOut = sigmoid(out[context])\n",
    "    negSamples = randomIdx(k, vocabSize, context)\n",
    "    _negSamples = [out[sample] for sample in negSamples]\n",
    "    e = -np.log(actOut) - np.sum(np.log(sigmoid(np.negative(_negSamples))))\n",
    "    #cost = np.concatenate(cost, e)\n",
    "    \n",
    "    \"\"\" calculating gradients for output vectors for both target and negative samples\n",
    "    calculating hidden layer error for each context word \"\"\"\n",
    "    delta = actOut - 1\n",
    "    errorHidden += delta * W_Output[:,context:context+1]\n",
    "    W_Output[:,context:context+1] -= learningRate * np.reshape(delta * emb,(emb.size,1))\n",
    "    for sample in negSamples:\n",
    "        delta = sigmoid(out[sample])\n",
    "        errorHidden += delta * W_Output[:,sample:sample+1]\n",
    "        W_Output[:,sample:sample+1] -= learningRate * np.reshape(delta * emb,(emb.size,1))\n",
    "    \n",
    "    return errorHidden\n",
    "    \n",
    "def skipgram(target,contextWords, vocabSize, learningRate, W_Embedding, W_Output):\n",
    "    \n",
    "    \"\"\"\n",
    "    will be called on each window with\n",
    "    target: Target word index\n",
    "    contextWords: Arrray of integers representing context words\n",
    "    \"\"\"\n",
    "    k = 10 #Number of negative samples\n",
    "    emb = W_Embedding[target]\n",
    "    out = np.matmul(emb,W_Output) # [1 x EmbSize].[EmbSize x VocabSize]\n",
    "    _predicted = []\n",
    "    EH = np.zeros(shape=(emb.size,1))\n",
    "    for context in contextWords:\n",
    "        #predicted = hotVector(context, vocabSize)\n",
    "        EH += negSamplingCostGradient(out, context, emb, vocabSize, learningRate, W_Output, k)\n",
    "        \n",
    "    #updating hidden layer input vector embedding\n",
    "    W_Embedding[target] -= learningRate * EH.T[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'enjoy': 1, u'S_END': 30, u'have': 3, u'tired': 4, u'ran': 5, u'is': 6, u'am': 7, u'see': 9, u'at': 10, u'want': 11, u'go': 12, u'tomorrow': 20, u'speak': 15, u'what': 17, u'how': 22, u'sun': 19, u'friends': 42, u'day': 43, u'graduate': 13, u'write': 16, u'to': 18, u'of': 51, u'enjoys': 24, u'has': 26, u'beach': 27, u'?': 28, u'nice': 53, u'dad': 2, u'be': 31, u'we': 32, u'good': 33, u'read': 8, u'student': 36, u'birth': 40, u'here': 37, u'every': 38, u'food': 39, u'mom': 21, u'date': 41, 'UNK': 0, u'come': 23, u'you': 25, u'I': 34, u'a': 44, u'boy': 45, u'store': 50, u'your': 14, u'name': 46, u'did': 47, u'S_START': 48, u'work': 49, u'can': 29, u'night': 35, u'the': 52, u'where': 54, u'are': 55}\n",
      "{u'comment': 1, u'votre': 2, u'peux': 3, u'aller': 4, u'dipl\\xf4m\\xe9': 5, u'appelez': 6, u'naissance': 7, u'allez': 8, u'peut': 9, u'au': 11, u'\\xe9crire': 12, u'passe': 13, u'veux': 14, u'suis': 25, u'ici': 17, u'est': 18, u'couru': 19, u'quelle': 20, u'la': 21, u'parler': 22, u'appr\\xe9cie': 24, u'avez': 16, u'demain': 26, u'chaque': 27, u'sais': 28, u'nourriture': 29, u'lire': 15, u'une': 59, u'S_END': 31, u'vous': 32, u'venez': 33, u'travail': 34, u'o\\xf9': 60, u'\\xe0': 47, u'nuit': 38, u'de': 37, u'papa': 36, u'\\xe9tudiant': 57, u'appr\\xe9ci\\xe9': 39, u'fatigue': 41, u'rendez': 42, u'amis': 61, u'date': 43, u'je': 44, 'UNK': 0, u'maman': 23, u'a': 46, u'on': 35, u'bonne': 58, u'\\xeatre': 10, u'\\xcates': 49, u'plage': 50, u'S_START': 51, u'journ\\xe9e': 52, u'venue': 53, u'venu': 55, u'gar\\xe7on': 56, u'le': 40, u'un': 54, u'?': 30, u'magasin': 45, u'soleil': 48}\n",
      "tokens ['S_START', 'have', 'a', 'good', 'day', 'at', 'work', 'S_END']\n",
      "tokens ['S_START', 'how', 'are', 'you', '?', 'S_END']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/preethikapachaiyappa/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:86: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/Users/preethikapachaiyappa/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:95: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/Users/preethikapachaiyappa/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:96: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/Users/preethikapachaiyappa/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:65: RuntimeWarning: overflow encountered in exp\n",
      "/Users/preethikapachaiyappa/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:89: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens ['S_START', 'what', 'is', 'your', 'name', '?', 'S_END']\n",
      "tokens ['S_START', 'what', 'is', 'your', 'date', 'of', 'birth', '?', 'S_END']\n",
      "tokens ['S_START', 'I', 'want', 'to', 'go', 'to', 'beach', 'S_END']\n",
      "tokens ['S_START', 'the', 'boy', 'ran', 'to', 'the', 'store', 'S_END']\n",
      "tokens ['S_START', 'every', 'boy', 'enjoys', 'the', 'sun', 'S_END']\n",
      "tokens ['S_START', 'can', 'we', 'be', 'friends', '?', 'S_END']\n",
      "tokens ['S_START', 'come', 'here', 'S_END']\n",
      "tokens ['S_START', 'I', 'am', 'tired', 'S_END']\n",
      "tokens ['S_START', 'see', 'you', 'tomorrow', 'S_END']\n",
      "tokens ['S_START', 'have', 'a', 'nice', 'night', 'S_END']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\" Creates word embeddings in vector space representation \"\"\"\n",
    "\n",
    "\"\"\" Feedforward Neural Net Language model \"\"\"\n",
    "#Input layer\n",
    "\n",
    "#Projection layer\n",
    "\n",
    "#Hidden layer\n",
    "\n",
    "#Output layer\n",
    "\n",
    "#Initialization\n",
    "fin='/Users/preethikapachaiyappa/Documents/MachineLearning/Data/English-small.txt'#/home/jazzycrazzy/PythonScripts/dataset.csv'\n",
    "fin1='/Users/preethikapachaiyappa/Documents/MachineLearning/Data/French-small.txt'\n",
    "fout = 'testfile.txt'\n",
    "fout1 = 'testfile1.txt'\n",
    "_vocab, words = dataset(fin, fout)\n",
    "_vocab_f, words_f = dataset(fin1, fout)\n",
    "_unigramTable = UnigramTable(_vocab, words)\n",
    "\n",
    "learningRate = 0.2\n",
    "vocabSize = len(words)\n",
    "vocabSize_f = len(words_f)\n",
    "emb_size = 10\n",
    "win_size = 2\n",
    "target = None\n",
    "contextWords = []\n",
    "\n",
    "print _vocab\n",
    "print _vocab_f\n",
    "\n",
    "# No need of hidden layer since when the embedding matrix is multiplied with hot vector \n",
    "#it essentially gives that embedding row\n",
    "W_Embedding = np.random.randn(vocabSize,emb_size) #Embedding matrix\n",
    "W_Output = np.random.randn(emb_size,vocabSize) #Outputlayer weight matrix Emb_size x Vocab\n",
    "\n",
    "fileIn = open(fin)\n",
    "for l in fileIn:\n",
    "    l = _start+' '+l+' '+_end\n",
    "    tokens = word_tokenize(l)\n",
    "    print 'tokens',tokens\n",
    "    for token in tokens:\n",
    "        if token in _vocab:\n",
    "            target = _vocab[token]\n",
    "            trgtIdx = tokens.index(token)\n",
    "            cntxtIdxs = range(trgtIdx-win_size, trgtIdx+win_size+1)\n",
    "            cntxtIdxs.remove(trgtIdx)\n",
    "            for idx in cntxtIdxs:\n",
    "                if idx >-1 and idx < len(tokens) and tokens[idx] in _vocab:\n",
    "                    contextWords = np.append(contextWords, _vocab[tokens[idx]])\n",
    "                else:\n",
    "                    contextWords = np.append(contextWords, _vocab['UNK']) \n",
    "            skipgram(target, contextWords, vocabSize, learningRate, W_Embedding, W_Output)\n",
    "print W_Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inSentence = []\n",
    "expSentence = []\n",
    "\n",
    "fileIn0 = open(fin)\n",
    "for l in fileIn0 :\n",
    "    l = _start+' '+l+' '+_end\n",
    "    tokens = word_tokenize(l)\n",
    "    inSent = []\n",
    "    for token in tokens :\n",
    "        target = _vocab[token]\n",
    "        vec = W_Embedding[target]\n",
    "        vec_list = vec.tolist()\n",
    "        inSent.append(vec_list)\n",
    "    inSentence.append(inSent)\n",
    "\n",
    "fileIn1 = open(fin1)\n",
    "for l in fileIn1 :\n",
    "    l = _start+' '+l+' '+_end\n",
    "    tokens = word_tokenize(l.decode('utf-8'))\n",
    "    expSent = []\n",
    "    for token in tokens :\n",
    "        target = _vocab_f[token]\n",
    "        expSent.append(target)\n",
    "    expSentence.append(expSent)\n",
    "\n",
    "#print inSentence\n",
    "#print expSentence\n",
    "        \n",
    "a = RNNlayer(10,vocabSize_f)\n",
    "a.train_Decoder_With_SGD(inSentence, expSentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
