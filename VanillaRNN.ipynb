{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Implementing decoder\"\"\"\n",
    "\n",
    "\"\"\"Imports\"\"\"\n",
    "import numpy as np\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "\"\"\"Global definitons\"\"\"\n",
    "_start = 'S_START'\n",
    "_end = 'S_END'\n",
    "_unk = 'UNK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" util definitions\"\"\"\n",
    "\n",
    "def hyperbolic(net):\n",
    "    return np.tanh(net)\n",
    "\n",
    "def relu(net):\n",
    "    return np.maximum(0,net)\n",
    "\n",
    "def softmax(net):\n",
    "    _exp = np.exp(net)\n",
    "    return _exp/np.sum(_exp)\n",
    "\n",
    "def predict(scores):\n",
    "    return np.argmax(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNlayer:\n",
    "    \n",
    "    \"\"\" \n",
    "    RNN nodes for decoder\n",
    "    \n",
    "    hidden state at time step t of decoder is conditioned on hidden state at time step t-1,\n",
    "    output at time step t-1 and context C from the encoder\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, inputSize, outputSize, bptt_truncate = 5, hiddenDim = 10):\n",
    "        \"\"\"\n",
    "        inputSize = dimensions of the context from encoder\n",
    "        outputSize = vocabulary size\n",
    "        hiddenDim = size of the hidden unit in RNN\n",
    "        bptt_truncate = truncate the number of time steps we calculate the gradient during backpropagation\n",
    "        \"\"\"\n",
    "        self.inputSize = inputSize\n",
    "        self.outputSize = outputSize\n",
    "        self.hiddenDim = hiddenDim\n",
    "        self. bptt_truncate = bptt_truncate\n",
    "        \n",
    "        self.w_in = np.random.uniform(-np.sqrt(1./inputSize), np.sqrt(1./inputSize),(hiddenDim, inputSize))\n",
    "        self.w_hh = np.random.uniform(-np.sqrt(1./hiddenDim), np.sqrt(1./hiddenDim),(hiddenDim, hiddenDim))\n",
    "        self.w_outH = np.random.uniform(-np.sqrt(1./hiddenDim), np.sqrt(1./hiddenDim),(outputSize, hiddenDim))\n",
    "        self.w_out = np.random.uniform(-np.sqrt(1./hiddenDim), np.sqrt(1./hiddenDim),(outputSize, hiddenDim))\n",
    "        \n",
    "    def forwardProp(self, context, expSent):\n",
    "        \"\"\"\n",
    "        context: calculated from encoder\n",
    "        expSent: word indices in target language vocabulary\n",
    "        \"\"\"\n",
    "        \n",
    "        #Total number of time steps equal to number of words in the sentence\n",
    "        T = len(expSent)\n",
    "        \n",
    "        #Saving all hidden states and outputs during forward propagation\n",
    "        _h = np.zeros((T,self.hiddenDim))\n",
    "        _o = np.zeros((T,self.outputSize))\n",
    "        \n",
    "        #Initializing initial output as the start token\n",
    "        _o[-1] = \n",
    "        \n",
    "        #For each time step calculating hidden state and output\n",
    "        for t in np.arange(T):\n",
    "            outIdx = predict(_o[t-1])\n",
    "            _h[t] = hyperbolic(self.w_in.dot(context) + self.w_hh.dot(_h[t-1]) + self.w_outH[:,outIdx:outIdx+1])\n",
    "            _o[t] = softmax(self.w_out.dot(_h[t]))\n",
    "            \n",
    "        return _o, _h\n",
    "    \n",
    "    def calculateLoss(self, context, expSentence):\n",
    "        \n",
    "        #For each sentence\n",
    "        o, h = self.forwardProp(context, expSentence)\n",
    "        #TODO recheck this part\n",
    "        correctPred = o[np.arange(len(expSentence)), expSentence]\n",
    "        #Loss for each sentence\n",
    "        l = -1 * np.sum(np.log(correctPred))\n",
    "        return l\n",
    "    \n",
    "    def calculateTotalLoss(self, contexts, expSentences):\n",
    "        \n",
    "        L = 0.0\n",
    "        for i in len(contexts):\n",
    "            L += self.calculateLoss(context[i], expSentences[i])\n",
    "            \n",
    "        return L\n",
    "    \n",
    "    def backPropTT(self, context, expSentence):\n",
    "        \n",
    "        # Total number of time steps equal to number of words in the sentence\n",
    "        T = len(expSentence)\n",
    "        \n",
    "        # Performing forward propagation\n",
    "        o, h = self.forwardProp(context, expSentence)\n",
    "        \n",
    "        # Defining gradient variables\n",
    "        dLdin = np.zeros(self.w_in.shape)\n",
    "        dLdhh = np.zeros(self.w_hh.shape)\n",
    "        dLdoutH = np.zeros(self.w_outH.shape)\n",
    "        dLdout = np.zeros(self.w_out.shape)\n",
    "        \n",
    "        # Calculating the difference between output and actual output\n",
    "        delta_o = o\n",
    "        delta_o[np.arange(T), expSentence] -= 1\n",
    "        \n",
    "        # Calculating gradients backwards through time\n",
    "        for t in np.arange(T)[::-1]:\n",
    "            #Output gradient is only dependent on time step t\n",
    "            dLdout += np.outer(delta_o[t], h[t])\n",
    "            # Initial delta calculation propagating gradients from output\n",
    "            delta_t = self.w_out.T.dot(delta_o[t]) * (1 - (h[t] ** 2))\n",
    "            # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "            for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "                # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "                # Add to gradients at each previous step\n",
    "                dLdhh += np.outer(delta_t, h[bptt_step-1])              \n",
    "                dLdin += np.outer(delta_t, context)\n",
    "                dLdoutH += np.outer(delta_t, o[bptt_step-1])\n",
    "                # Update delta for next step dL/dz at t-1\n",
    "                delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "            \"\"\"TODO review backprop implementation\"\"\"\n",
    "            \n",
    "        return dLdin, dLdhh, dLdoutH, dLdout\n",
    "    \n",
    "    def sgd_step(self, context, expSentence, learningRate):\n",
    "        \n",
    "        \"\"\" Performs a single stochastic gradient step\"\"\"\n",
    "        \n",
    "        # Calculating gradients\n",
    "        dLdin, dLdhh, dLdoutH, dLdout = self.backPropTT(context, expSentence)\n",
    "        \n",
    "        # Updating parameters\n",
    "        self.w_in -= learningRate * dLdin\n",
    "        self.w_hh -= learningRate * dLdhh\n",
    "        self.w_outH -= learningRate * dLdoutH\n",
    "        self.w_out -= learningRate * dLdout\n",
    "        \n",
    "    def train_Decoder_With_SGD(self, X_train, Y_train, learningRate = 0.05, nepochs = 10):\n",
    "        \"\"\"TODO evaluate losses and update learning rate if required\"\"\"\n",
    "        for epoch in range(nepochs):\n",
    "            for i in range(len(Y_train)):\n",
    "                self.sgd_step(X_train[i], Y_train[i], learningRate)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabSize = None\n",
    "embSize = None\n",
    "\n",
    "W_out = np.random.randn(vocabSize, embSize)\n",
    "W_hh = np.random.randn(embSize, embSize)\n",
    "W_in = np.random.randn(embSize,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a = np.array([[1,2,3],[4,5,6]])\n",
    "a[np.arange(2),[1,2]] -= 1\n",
    "print a[1].T\n",
    "print np.outer(a[0],a[1].T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
