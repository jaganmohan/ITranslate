{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Imports \"\"\"\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"Global definitons\"\"\"\n",
    "_start = 'S_START'\n",
    "_end = 'S_END'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WordItem:\n",
    "    def __init__(self,word,count=0):\n",
    "        self.word = word\n",
    "        self.count = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Word preprocessing \"\"\"\n",
    "def dataset(_fi='/home/jazzycrazzy/PythonScripts/dataset.csv', _fo = 'testfile.txt'):\n",
    "    file_in = open(_fi)\n",
    "    #file_out = open(_fo,'wb')\n",
    "\n",
    "    words = [] #stores unique words encountered in the document as WordItem objects\n",
    "    _dict = {} #temporary dictionary to maintain count of each word\n",
    "    \n",
    "    _dict['UNK'] = 0\n",
    "\n",
    "    for l in file_in:\n",
    "        #file_out.write(l+'\\n')\n",
    "        l = _start+' '+l+' '+_end\n",
    "        split = word_tokenize(l)\n",
    "        for w in split:\n",
    "            if len(w)==0:\n",
    "                continue\n",
    "            elif len(w) > 15: #if word's length is greater than 15 counting it as unknown\n",
    "                _dict['UNK'] += 1\n",
    "                continue\n",
    "            if w not in _dict:\n",
    "                _dict[w] = 1\n",
    "            _dict[w] += 1\n",
    "            \n",
    "    _vocab = {} #dictionary with words as keys and values as indices of them in 'word' list\n",
    "    _vocab['UNK'] = len(words)\n",
    "    words.append(WordItem('UNK',_dict['UNK']))\n",
    "    for k,v in _dict.iteritems():\n",
    "        if v > 9 and k != 'UNK':\n",
    "            _vocab[k] = len(words)\n",
    "            words.append(WordItem(k,v))\n",
    "        else:\n",
    "            words[0].count += 1\n",
    "    \n",
    "    #cleaning up unnecessary memory\n",
    "    del _dict\n",
    "    file_in.close()\n",
    "    #file_out.close()\n",
    "    \n",
    "    return _vocab, words\n",
    "\n",
    "def UnigramTable(_vocab, words):\n",
    "    \"\"\" Calculates probabilities based on count of each word present\"\"\"\n",
    "    pow = 0.75\n",
    "    totalFreqPow = 0.0\n",
    "    unigramTable = {}\n",
    "    \n",
    "    l = [words[i].count**pow for i in range(len(_vocab))]\n",
    "    totalFreqPow = np.sum(l)\n",
    "    \n",
    "    for i in range(len(_vocab)):\n",
    "        unigramTable[i] = (words[i].count**pow)/totalFreqPow\n",
    "    \n",
    "    del l\n",
    "    return unigramTable\n",
    "\n",
    "def hotVector(wordIndex,vocabSize):\n",
    "    \"\"\" Returns hot vector representation of a word \"\"\"\n",
    "    hVector = np.zeros(vocabSize)\n",
    "    hVector[wordIndex-1] = 1\n",
    "    return hVector\n",
    "\n",
    "def softmax(net):\n",
    "    \"\"\" calculates softmax score - target score normalized with noise scores and calculated as probability\"\"\"\n",
    "    _exp = np.exp(net)\n",
    "    return _exp/np.sum(_exp)\n",
    "\n",
    "def sigmoid(net):\n",
    "    \"\"\" Applies sigmoid logistic function on net \"\"\"\n",
    "    return 1.0/(1+np.exp(-net))\n",
    "\n",
    "def randomIdx(k, vocabSize, current):\n",
    "    \"\"\" Returns k indices from with unigram table randomly with respect to each word's probablity \"\"\"\n",
    "    global _unigramTable\n",
    "    idxs = list(np.random.choice(vocabSize, k+1, False, p = _unigramTable.values()))\n",
    "    if current in idxs:\n",
    "        idxs.remove(current)\n",
    "    else:\n",
    "        del idxs[-1]\n",
    "    return idxs\n",
    "    \n",
    "def softmaxCostGradient(net, target):\n",
    "    prob = softmax(net)\n",
    "    \n",
    "    \n",
    "def negSamplingCostGradient(out, context, emb, vocabSize, learningRate, W_Output, k = 10):\n",
    "    \n",
    "    #cost = []\n",
    "    errorHidden = np.zeros(shape=(emb.size,1))\n",
    "    \n",
    "    actOut = sigmoid(out[context])\n",
    "    negSamples = randomIdx(k, vocabSize, context)\n",
    "    _negSamples = [out[sample] for sample in negSamples]\n",
    "    e = -np.log(actOut) - np.sum(np.log(sigmoid(np.negative(_negSamples))))\n",
    "    #cost = np.concatenate(cost, e)\n",
    "    \n",
    "    \"\"\" calculating gradients for output vectors for both target and negative samples\n",
    "    calculating hidden layer error for each context word \"\"\"\n",
    "    delta = actOut - 1\n",
    "    errorHidden += delta * W_Output[:,context:context+1]\n",
    "    W_Output[:,context:context+1] -= learningRate * np.reshape(delta * emb,(emb.size,1))\n",
    "    for sample in negSamples:\n",
    "        delta = sigmoid(out[sample])\n",
    "        errorHidden += delta * W_Output[:,sample:sample+1]\n",
    "        W_Output[:,sample:sample+1] -= learningRate * np.reshape(delta * emb,(emb.size,1))\n",
    "    \n",
    "    return errorHidden\n",
    "    \n",
    "def skipgram(target,contextWords, vocabSize, learningRate, W_Embedding, W_Output):\n",
    "    \n",
    "    \"\"\"\n",
    "    will be called on each window with\n",
    "    target: Target word index\n",
    "    contextWords: Arrray of integers representing context words\n",
    "    \"\"\"\n",
    "    k = 10 #Number of negative samples\n",
    "    emb = W_Embedding[target]\n",
    "    out = np.matmul(emb,W_Output) # [1 x EmbSize].[EmbSize x VocabSize]\n",
    "    _predicted = []\n",
    "    EH = np.zeros(shape=(emb.size,1))\n",
    "    for context in contextWords:\n",
    "        #predicted = hotVector(context, vocabSize)\n",
    "        EH += negSamplingCostGradient(out, context, emb, vocabSize, learningRate, W_Output, k)\n",
    "        \n",
    "    #updating hidden layer input vector embedding\n",
    "    W_Embedding[target] -= learningRate * EH.T[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens ['S_START', '9/11', 'in', 'Perspective', 'S_END']\n",
      "tokens ['S_START', 'NEW', 'YORK', '\\xe2\\x80\\x93', 'It', 'was', 'a', 'decade', 'ago', 'that', '19', 'terrorists', 'took', 'control', 'of', 'four', 'planes', ',', 'flew', 'two', 'into', 'the', 'twin', 'towers', 'of', 'the', 'World', 'Trade', 'Center', ',', 'hit', 'the', 'Pentagon', 'with', 'a', 'third', ',', 'and', 'crashed', 'the', 'fourth', 'in', 'a', 'field', 'in', 'Pennsylvania', 'after', 'passengers', 'resisted', 'and', 'made', 'it', 'impossible', 'for', 'the', 'terrorists', 'to', 'complete', 'their', 'malevolent', 'mission', '.', 'S_END']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jazzycrazzy/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:91: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/home/jazzycrazzy/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:100: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/home/jazzycrazzy/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:101: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/home/jazzycrazzy/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:70: RuntimeWarning: overflow encountered in exp\n",
      "/home/jazzycrazzy/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:94: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens ['S_START', 'In', 'a', 'matter', 'of', 'hours', ',', 'more', 'than', '3,000', 'innocent', 'people', ',', 'mostly', 'Americans', ',', 'but', 'also', 'people', 'from', '115', 'other', 'countries', ',', 'had', 'their', 'lives', 'suddenly', 'and', 'violently', 'taken', 'from', 'them', '.', 'S_END']\n",
      "tokens ['S_START', 'September', '11', ',', '2001', ',', 'was', 'a', 'terrible', 'tragedy', 'by', 'any', 'measure', ',', 'but', 'it', 'was', 'not', 'a', 'historical', 'turning', 'point', '.', 'S_END']\n",
      "tokens ['S_START', 'It', 'did', 'not', 'herald', 'a', 'new', 'era', 'of', 'international', 'relations', 'in', 'which', 'terrorists', 'with', 'a', 'global', 'agenda', 'prevailed', ',', 'or', 'in', 'which', 'such', 'spectacular', 'terrorist', 'attacks', 'became', 'commonplace', '.', 'S_END']\n",
      "tokens ['S_START', 'On', 'the', 'contrary', ',', '9/11', 'has', 'not', 'been', 'replicated', '.', 'S_END']\n",
      "tokens ['S_START', 'Despite', 'the', 'attention', 'devoted', 'to', 'the', '\\xe2\\x80\\x9cGlobal', 'War', 'on', 'Terrorism', ',', '\\xe2\\x80\\x9d', 'the', 'most', 'important', 'developments', 'of', 'the', 'last', 'ten', 'years', 'have', 'been', 'the', 'introduction', 'and', 'spread', 'of', 'innovative', 'information', 'technologies', ',', 'globalization', ',', 'the', 'wars', 'in', 'Iraq', 'and', 'Afghanistan', ',', 'and', 'the', 'political', 'upheavals', 'in', 'the', 'Middle', 'East.\\xc2\\xa0', 'S_END']\n",
      "tokens ['S_START', 'As', 'for', 'the', 'future', ',', 'it', 'is', 'much', 'more', 'likely', 'to', 'be', 'defined', 'by', 'the', 'United', 'States\\xe2\\x80\\x99', 'need', 'to', 'put', 'its', 'economic', 'house', 'in', 'order', ';', 'China\\xe2\\x80\\x99s', 'trajectory', 'within', 'and', 'beyond', 'its', 'borders', ';', 'and', 'the', 'ability', 'of', 'the', 'world\\xe2\\x80\\x99s', 'governments', 'to', 'cooperate', 'on', 'restoring', 'economic', 'growth', ',', 'stemming', 'the', 'spread', 'of', 'nuclear', 'weapons', ',', 'and', 'meeting', 'energy', 'and', 'environmental', 'challenges', '.', 'S_END']\n",
      "tokens ['S_START', 'It', 'is', 'and', 'would', 'be', 'wrong', 'to', 'make', 'opposition', 'to', 'terrorism', 'the', 'centerpiece', 'of', 'what', 'responsible', 'governments', 'do', 'in', 'the', 'world', '.', 'S_END']\n",
      "tokens ['S_START', 'Terrorists', 'continue', 'to', 'be', 'outliers', 'with', 'limited', 'appeal', 'at', 'best', '.', 'S_END']\n",
      "tokens ['S_START', 'They', 'can', 'destroy', 'but', 'not', 'create', '.', 'S_END']\n",
      "tokens ['S_START', 'It', 'is', 'worth', 'noting', 'that', 'the', 'people', 'who', 'went', 'into', 'the', 'streets', 'of', 'Cairo', 'and', 'Damascus', 'calling', 'for', 'change', 'were', 'not', 'shouting', 'the', 'slogans', 'of', 'Al', 'Qaeda', 'or', 'supporting', 'its', 'agenda', '.', 'S_END']\n",
      "tokens ['S_START', 'Moreover', ',', 'measures', 'have', 'been', 'implemented', 'to', 'push', 'back', ',', 'successfully', ',', 'against', 'terrorists', '.', 'S_END']\n",
      "tokens ['S_START', 'Intelligence', 'assets', 'have', 'been', 'redirected', '.', 'S_END']\n",
      "tokens ['S_START', 'Borders', 'have', 'been', 'made', 'more', 'secure', 'and', 'societies', 'more', 'resilient', '.', 'S_END']\n",
      "tokens ['S_START', 'International', 'cooperation', 'has', 'increased', 'markedly', ',', 'in', 'part', 'because', 'governments', 'that', 'can', 'not', 'agree', 'on', 'many', 'things', 'can', 'agree', 'on', 'the', 'need', 'to', 'cooperate', 'in', 'this', 'area', '.', 'S_END']\n",
      "tokens ['S_START', 'Military', 'force', 'has', 'played', 'a', 'role', 'as', 'well', '.', 'S_END']\n",
      "tokens ['S_START', 'Al', 'Qaeda', 'lost', 'its', 'base', 'in', 'Afghanistan', 'when', 'the', 'Taliban', 'government', 'that', 'had', 'provided', 'it', 'sanctuary', 'was', 'ousted', 'from', 'power', '.', 'S_END']\n",
      "tokens ['S_START', 'Osama', 'bin-Laden', 'was', 'finally', 'found', 'and', 'killed', 'by', 'US', 'Special', 'Forces', 'in', 'the', 'suburbs', 'of', 'Islamabad', '.', 'S_END']\n",
      "tokens ['S_START', 'Drones', '\\xe2\\x80\\x93', 'unmanned', 'aircraft', 'that', 'are', 'remotely', 'steered', '\\xe2\\x80\\x93', 'have', 'proven', 'to', 'be', 'effective', 'in', 'killing', 'a', 'significant', 'number', 'of', 'terrorists', ',', 'including', 'many', 'of', 'the', 'most', 'important', 'leaders', '.', 'S_END']\n",
      "tokens ['S_START', 'Weak', 'governments', 'can', 'be', 'made', 'stronger', ';', 'governments', 'that', 'tolerate', 'or', 'support', 'terrorism', 'must', 'be', 'held', 'accountable', '.', 'S_END']\n",
      "tokens ['S_START', 'But', 'progress', 'is', 'not', 'to', 'be', 'confused', 'with', 'victory', '.', 'S_END']\n",
      "tokens ['S_START', 'Terrorists', 'and', 'terrorism', 'can', 'not', 'be', 'eliminated', 'any', 'more', 'than', 'we', 'can', 'rid', 'the', 'world', 'of', 'disease', '.', 'S_END']\n",
      "tokens ['S_START', 'There', 'will', 'always', 'be', 'those', 'who', 'will', 'resort', 'to', 'force', 'against', 'innocent', 'men', ',', 'women', ',', 'and', 'children', 'in', 'pursuit', 'of', 'political', 'goals', '.', 'S_END']\n",
      "tokens ['S_START', 'Indeed', ',', 'terrorists', 'are', 'advancing', 'in', 'some', 'areas', '.', 'S_END']\n",
      "tokens ['S_START', 'Pakistan', 'remains', 'a', 'sanctuary', 'for', 'Al', 'Qaeda', 'and', 'some', 'of', 'the', 'world\\xe2\\x80\\x99s', 'other', 'most', 'dangerous', 'terrorists', '.', 'S_END']\n",
      "tokens ['S_START', 'A', 'mixture', 'of', 'instability', ',', 'government', 'weakness', ',', 'and', 'ideology', 'in', 'countries', 'such', 'as', 'Yemen', ',', 'Libya', ',', 'Somalia', ',', 'and', 'Nigeria', 'are', 'providing', 'fertile', 'territory', 'for', 'terrorists', 'to', 'organize', ',', 'train', ',', 'and', 'mount', 'operations', '\\xe2\\x80\\x93', 'much', 'as', 'they', 'did', 'in', 'Afghanistan', 'did', 'a', 'decade', 'ago', '.', 'S_END']\n",
      "tokens ['S_START', 'New', 'groups', 'constantly', 'emerge', 'from', 'the', 'ruins', 'of', 'old', 'ones', '.', 'S_END']\n",
      "tokens ['S_START', 'There', 'is', 'also', 'a', 'growing', 'danger', 'of', 'homegrown', 'terrorism', '.', 'S_END']\n",
      "tokens ['S_START', 'We', 'have', 'seen', 'it', 'in', 'Great', 'Britain', 'and', 'the', 'US', '.', 'S_END']\n",
      "tokens ['S_START', 'The', 'Internet', ',', 'one', 'of', 'the', 'great', 'inventions', 'of', 'the', 'modern', 'Western', 'world', ',', 'has', 'shown', 'itself', 'to', 'be', 'a', 'weapon', 'that', 'can', 'be', 'used', 'to', 'incite', 'and', 'train', 'those', 'who', 'wish', 'to', 'cause', 'harm', 'to', 'that', 'world', '.', 'S_END']\n",
      "tokens ['S_START', 'The', 'question', 'raised', 'in', 'October', '2003', 'by', 'then', 'US', 'Secretary', 'of', 'Defense', 'Donald', 'Rumsfeld', 'is', 'no', 'less', 'relevant', 'today', ':', '\\xe2\\x80\\x9cAre', 'we', 'capturing', ',', 'killing', ',', 'or', 'deterring', 'and', 'dissuading', 'more', 'terrorists', 'every', 'day', 'than', 'the', 'madrassas', 'and', 'the', 'radical', 'clerics', 'are', 'recruiting', ',', 'training', ',', 'and', 'deploying', 'against', 'us', '?', '\\xe2\\x80\\x9d', 'All', 'things', 'being', 'equal', ',', 'we', 'probably', 'are', '.', 'S_END']\n",
      "tokens ['S_START', 'But', 'even', 'small', 'terrorist', 'successes', 'are', 'costly', 'in', 'terms', 'of', 'lives', ',', 'money', ',', 'and', 'making', 'open', 'societies', 'less', 'so', '.', 'S_END']\n",
      "tokens ['S_START', 'What', 'is', 'to', 'be', 'done', '?', 'S_END']\n",
      "tokens ['S_START', 'Alas', ',', 'there', 'is', 'no', 'single', 'or', 'silver', 'bullet', '.', 'S_END']\n",
      "tokens ['S_START', 'The', 'establishment', 'of', 'a', 'Palestinian', 'state', 'will', 'not', 'be', 'enough', 'for', 'those', 'terrorists', 'who', 'want', 'to', 'see', 'the', 'elimination', 'of', 'the', 'Jewish', 'state', ',', 'any', 'more', 'than', 'reaching', 'a', 'compromise', 'over', 'Kashmir', 'will', 'satisfy', 'those', 'Pakistan-based', 'terrorists', 'with', 'bigger', 'agendas', 'vis-\\xc3\\xa0-vis', 'India', '.', 'S_END']\n",
      "tokens ['S_START', 'Reducing', 'unemployment', 'is', 'desirable', ',', 'of', 'course', ',', 'but', 'many', 'terrorists', 'do', 'not', 'come', 'from', 'poverty', '.', 'S_END']\n",
      "tokens ['S_START', 'Helping', 'to', 'make', 'societies', 'in', 'the', 'Middle', 'East', 'and', 'elsewhere', 'more', 'democratic', 'might', 'reduce', 'the', 'alienation', 'that', 'can', 'lead', 'to', 'radicalism', 'and', 'worse', ',', 'but', 'this', 'is', 'easier', 'said', 'than', 'done', '.', 'S_END']\n",
      "tokens ['S_START', 'Of', 'course', ',', 'we', 'want', 'to', 'continue', 'to', 'find', 'ways', 'to', 'make', 'ourselves', 'less', 'vulnerable', 'and', 'terrorists', 'more', 'so', '.', 'S_END']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jazzycrazzy/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:123: RuntimeWarning: overflow encountered in add\n",
      "/home/jazzycrazzy/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:104: RuntimeWarning: overflow encountered in add\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens ['S_START', 'But', 'what', 'may', 'be', 'most', 'important', ',', 'particularly', 'in', 'the', 'Arab', 'and', 'Islamic', 'communities', ',', 'is', 'to', 'end', 'any', 'acceptance', 'of', 'terrorism', '.', 'S_END']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jazzycrazzy/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:123: RuntimeWarning: invalid value encountered in add\n",
      "/home/jazzycrazzy/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:100: RuntimeWarning: invalid value encountered in multiply\n",
      "/home/jazzycrazzy/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:101: RuntimeWarning: invalid value encountered in multiply\n",
      "/home/jazzycrazzy/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:104: RuntimeWarning: invalid value encountered in multiply\n",
      "/home/jazzycrazzy/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:105: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens ['S_START', 'The', 'Nigerian', 'father', 'who', 'warned', 'the', 'US', 'embassy', 'in', 'Lagos', 'that', 'he', 'feared', 'what', 'his', 'own', 'son', 'might', 'do', '\\xe2\\x80\\x93', 'before', 'that', 'same', 'young', 'man', 'attempted', 'to', 'detonate', 'a', 'bomb', 'aboard', 'a', 'flight', 'to', 'Detroit', 'on', 'Christmas', 'Day', '2009', '\\xe2\\x80\\x93', 'is', 'an', 'example', 'of', 'just', 'this', '.', 'S_END']\n",
      "tokens ['S_START', 'Only', 'when', 'more', 'parents', ',', 'teachers', ',', 'and', 'community', 'leaders', 'behave', 'likewise', 'will', 'recruitment', 'of', 'terrorists', 'dry', 'up', 'and', 'law-enforcement', 'authorities', 'receive', 'full', 'cooperation', 'from', 'the', 'populations', 'they', 'police', '.', 'S_END']\n",
      "tokens ['S_START', 'Terrorism', 'must', 'lose', 'its', 'legitimacy', 'among', 'those', 'who', 'have', 'historically', 'supported', 'or', 'tolerated', 'it', 'before', 'it', 'will', 'lose', 'its', 'potency', '.', 'S_END']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\" Creates word embeddings in vector space representation \"\"\"\n",
    "\n",
    "\"\"\" Feedforward Neural Net Language model \"\"\"\n",
    "#Input layer\n",
    "\n",
    "#Projection layer\n",
    "\n",
    "#Hidden layer\n",
    "\n",
    "#Output layer\n",
    "\n",
    "#Initialization\n",
    "fin='/home/jazzycrazzy/MTData/English/9-11-in-perspective.txt'#/home/jazzycrazzy/PythonScripts/dataset.csv'\n",
    "fout = 'testfile.txt'\n",
    "_vocab, words = dataset(fin, fout)\n",
    "_unigramTable = UnigramTable(_vocab, words)\n",
    "\n",
    "learningRate = 0.2\n",
    "vocabSize = len(words)\n",
    "emb_size = 10\n",
    "win_size = 2\n",
    "target = None\n",
    "contextWords = []\n",
    "\n",
    "#print _vocab\n",
    "\n",
    "\n",
    "# No need of hidden layer since when the embedding matrix is multiplied with hot vector \n",
    "#it essentially gives that embedding row\n",
    "W_Embedding = np.random.randn(vocabSize,emb_size) #Embedding matrix\n",
    "W_Output = np.random.randn(emb_size,vocabSize) #Outputlayer weight matrix Emb_size x Vocab\n",
    "\n",
    "fileIn = open(fin)\n",
    "for l in fileIn:\n",
    "    l = _start+' '+l+' '+_end\n",
    "    tokens = word_tokenize(l)\n",
    "    print 'tokens',tokens\n",
    "    for token in tokens:\n",
    "        if token in _vocab:\n",
    "            target = _vocab[token]\n",
    "            trgtIdx = tokens.index(token)\n",
    "            cntxtIdxs = range(trgtIdx-win_size, trgtIdx+win_size+1)\n",
    "            cntxtIdxs.remove(trgtIdx)\n",
    "            for idx in cntxtIdxs:\n",
    "                if idx >-1 and idx < len(tokens) and tokens[idx] in _vocab:\n",
    "                    contextWords = np.append(contextWords, _vocab[tokens[idx]])\n",
    "                else:\n",
    "                    contextWords = np.append(contextWords, _vocab['UNK']) \n",
    "            skipgram(target, contextWords, vocabSize, learningRate, W_Embedding, W_Output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"print _unigramTable\n",
    "print words[0].word,words[0].count\n",
    "print _vocab.values()[:10]\n",
    "print _vocab.keys()[:10]\n",
    "print words[_vocab.get('UNK')].count\n",
    "\n",
    "print _vocab\n",
    "#print W_Embedding\n",
    "fig = plt.figure()\n",
    "plt.scatter(W_Embedding[:,0:1], W_Embedding[:,1:2], W_Embedding[:,2:3])\n",
    "plt.show()\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
